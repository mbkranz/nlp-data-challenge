{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Fetch Rewards - Data Challenge - Frankenstein Hidden Sentences\n",
    " \n",
    " - this notebook gives the \"prototype\" built for the scraper. From this prototype, the next step is to make a ScraperClass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrankensteinScraper:\n",
    "    #bookend_symbols\n",
    "    #punctuation\n",
    "    def __init__(self,doc):\n",
    "        self._raw_doc = doc\n",
    "        \n",
    "    def tokenize():\n",
    "        pass\n",
    "    def make_bag_of_words():\n",
    "        pass\n",
    "    \n",
    "    def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/michaelkranz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str = '''\n",
    "Letter 1\n",
    "To Mrs. Saville, England. <Scrapers<\n",
    "St. Petersburgh, Dec. 11th, 17—.\n",
    "You will rejoice to hear that no disaster has accompanied the commencement of an enterprise which you have regarded with such evil forebodings. I arrived here yesterday, and my first task is to assure my $You$ dear sister of my welfare and increasing confidence in the success of my undertaking.\n",
    "I am already far north of London, and as I walk in the streets of Petersburgh, I feel a cold northern breeze play upon my cheeks, which braces my nerves and fills me with delight. Do you understand this feeling? This breeze, which has travelled from the regions towards which I am advancing, gives me a foretaste of those icy climes. Inspirited by this wind of promise, my daydreams become more fervent and vivid. I try in vain to be persuaded that the pole is the seat of frost and desolation; it ever presents $are$ itself to my imagination as the region of beauty and delight. There, Margaret, the sun is for ever visible, its broad disk just skirting the horizon and diffusing a perpetual splendour. There—for with your leave, my sister, I will put some trust in preceding navigators—there snow and frost are banished; and, sailing over a calm sea, we may be wafted to a land surpassing *You're* in wonders and in beauty every region hitherto discovered on the habitable globe. Its productions and features may be without example, as the phenomena of the heavenly bodies undoubtedly are in those undiscovered solitudes. What may not be expected in a country of eternal light? I may there discover the wondrous power which attracts the needle and may regulate a thousand celestial observations that require only this voyage to render their seeming eccentricities consistent for ever. I shall satiate my ardent curiosity with the sight of a part of the world never before visited, and may tread a land never before imprinted by the foot of man. These are my enticements, and they are sufficient to conquer all fear of danger or death and to induce me to commence this laborious voyage with the joy a child feels when he embarks in a little boat, with his holiday mates, on an expedition of discovery up his native river. But supposing all these conjectures to be false, you cannot contest the inestimable benefit which I shall confer on all mankind, to the last generation, by discovering a passage near the pole to those countries, to reach which at present so many months are requisite; or by ascertaining the secret of the magnet, which, if at all possible, can only be effected by an undertaking such as mine.\n",
    "These reflections have dispelled the agitation with which I began my letter, and I feel my heart glow with an enthusiasm which elevates me to heaven, for nothing contributes so much to tranquillise the mind as a steady purpose—a point on which the soul may fix its intellectual eye. This expedition has been the favourite dream of my early years. #Found# I have read with ardour the accounts of the various voyages which have been made in the prospect of arriving at the North Pacific Ocean through the seas which surround the pole. You may remember that a history of all the voyages made for purposes of discovery composed the whole of our good Uncle Thomas’ library. My education was neglected, yet I was passionately fond of reading. These volumes were my study day and night, and my familiarity with them increased that regret which I had felt, as a child, on learning that my father’s dying injunction had forbidden my uncle to allow me to embark in a seafaring life.\n",
    "These visions faded when I perused, for the first time, those poets whose effusions entranced my soul and lifted it to heaven. I also became a poet and for one year lived in a paradise of my own creation; I imagined that I also might obtain a niche in the temple where the names of Homer and Shakespeare are consecrated. You are well acquainted with *killin* my failure and how heavily I bore the disappointment. But just at that time I inherited the fortune of my cousin, and my thoughts were turned into the channel of their earlier bent.\n",
    "Six years have passed since I resolved on my present undertaking. I can, even now, remember the hour from which I dedicated myself to this great enterprise. I commenced by inuring my body to hardship. I accompanied the whale-fishers on several expeditions to the North Sea; I voluntarily endured cold, famine, thirst, and want of sleep; I often #another# worked harder than the common sailors during the day and devoted my nights to the study of mathematics, the theory of medicine, and those branches of physical science from which a naval adventurer might derive the greatest practical advantage. Twice I actually hired myself as an under-mate in a Greenland whaler, and acquitted myself to admiration. I must own I felt a little proud when my captain offered me the second dignity in the vessel and entreated me to remain with the greatest earnestness, so valuable did he consider my services.\n",
    "And now, dear <for< Margaret, do I not deserve to accomplish some great purpose? My life might have been passed in ease and luxury, but I preferred glory to every enticement that wealth placed in my path. Oh, that some encouraging voice would answer in the affirmative! My courage and my resolution is firm; but my hopes fluctuate, and my spirits are often depressed. I am about to proceed on a long and difficult voyage, the emergencies of which will demand all my fortitude: I am required not only to raise the spirits of others, but sometimes to sustain my own, when theirs are failing.\n",
    "This is the most favourable period for travelling in Russia. They fly quickly over the snow in their sledges; the motion is pleasant, and, in my opinion, far more agreeable than that of an English stagecoach. The $amazing!$ cold is not excessive, if you are wrapped in furs—a dress which I have already adopted, for there is a great difference between walking the deck and remaining seated motionless for hours, when no exercise prevents the blood from actually freezing in your veins. I have no ambition to lose my life on the post-road between St. Petersburgh and Archangel. <the<\n",
    "I shall depart for the latter town in a fortnight or three weeks; and my intention is to hire a ship there, which can easily be done by paying the insurance for the owner, *it!* and to engage as many sailors as I think necessary among those who are accustomed to the whale-fishing. I do not intend to sail until the month of June; and when shall I return? Ah, dear sister, how can I answer this question? If I succeed, many, many months, perhaps years, will pass before you and I may meet. If I fail, you will see me again soon, or never.\n",
    "Farewell, my dear, excellent Margaret. Heaven #one!# shower down blessings on you, and save me, that I may again and again testify my gratitude for all your love and kindness.\n",
    "Your affectionate brother,\n",
    "R. Walton<win!<\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#core python solution, nltk, or spacy or does it not matter?\n",
    "bookend_symbols = {\n",
    "    \"ASTERIX\":'*',\n",
    "    \"LESSTHAN\":'<',\n",
    "    \"DOLLAR\":'$',\n",
    "    \"HASHTAG\":'#'\n",
    "}\n",
    "bookend_chars = list(bookend_symbols.values())\n",
    "punctuation_chars = ['?','!','.',',',';'] #sentence tokenization\n",
    "\n",
    "# word_split_chars = ['—',' '] #word tokenization\n",
    "# other_chars = ['\\n'] \n",
    "\n",
    "# split_chars = [\n",
    "#     \"\\\\\" + char \n",
    "#     for char in (punctuation_chars + \n",
    "#                  word_split_chars +\n",
    "#                  other_chars +\n",
    "#                  bookend_chars)\n",
    "# ]\n",
    "# split_regex = r\"|\".join(split_chars)\n",
    "               \n",
    "# text_tokenized = [\n",
    "#     x \n",
    "#     for x in re.split(split_regex,\n",
    "#                       text_str) \n",
    "#     if x!=''\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assumes \"punctuation\" entails both classic punctuation characters and bookend characters\n",
    "text_tokenized = [\n",
    "    word\n",
    "    for word in word_tokenize(text_str.lower())\n",
    "    if word not in punctuation_chars and word not in bookend_chars\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store all distinct words in the text in a dictionary and keep track of the word counts (make sure to strip all punctuation and lower the case of the words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Dictionary([text_tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.(text_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given only one document (and not multiple), \n",
    "#can use Counter rather than other tools like gensim to make Dictionary and .doc2bow   \n",
    "text_bow_df = pd.Series(Counter(text_tokenized)).reset_index()\n",
    "text_bow_df.columns = ['word','word_count']\n",
    "text_bow_df['word_length'] = text_bow_df.word.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the distinct word dictionary as a JSON file (call it 'word_count_dict.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Number of Words (Count)\\nat a Word Length')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot a histogram of the count of words by word length for all words in the document\n",
    "text_word_count_by_word_length = text_bow_df.groupby('word_length').word_count.sum()\n",
    "\n",
    "\n",
    "fig_word_count_by_word_length = text_word_count_by_word_length.plot(kind='bar')\n",
    "plt.title(\"Count of Words by Word Length \")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Number of Words (Count)\\nat a Word Length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 4 hidden sentences together as a list of lists\n",
    "#Remove the 4 hidden sentences from the original text file\n",
    "\n",
    "hidden_sentence_indices = {}\n",
    "hidden_sentence_words = {}\n",
    "for name,symbol in bookend_symbols.items():\n",
    "    tmp_index_list = []\n",
    "    tmp_text_list = []\n",
    "    for match in re.finditer(f\"\\{symbol}.*\\{symbol}\",text_str):\n",
    "        match_indices = match.span()\n",
    "        start,end = match_indices\n",
    "        match_text = text_str[start:end]\n",
    "        tmp_index_list.append(match_indices)\n",
    "        tmp_text_list.append(match_text.replace(symbol,''))\n",
    "    hidden_sentence_indices.update({name:tmp_index_list})\n",
    "    hidden_sentence_words.update({name:tmp_text_list})\n",
    "\n",
    "#for removal, dont care about bookend type so flatten/combine lists\n",
    "flattened_hidden_sentence_indices = [\n",
    "    i \n",
    "    for bookend_indices in hidden_sentence_indices.values() \n",
    "    for i in bookend_indices\n",
    "]\n",
    "#need to put bookend indices in order of which they appear given sequential nature of cleaning approach\n",
    "flattened_hidden_sentence_indices = (\n",
    "    pd.DataFrame(flattened_hidden_sentence_indices)\n",
    "    .sort_values(0)\n",
    "    .to_records(index=False)\n",
    ")\n",
    "#get the substring from right after last bookend to beginning of current bookend\n",
    "clean_string = '' #initiate clean string to be appended\n",
    "clean_start_index = 0 #initiate start index for loop\n",
    "for start,end in flattened_hidden_sentence_indices:\n",
    "    clean_substr = text_str[clean_start_index:start]\n",
    "    clean_string+=clean_substr\n",
    "    clean_start_index = end\n",
    "#add rest of clean text after last bookend\n",
    "clean_string+=text_str[clean_start_index:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ASTERIX': [\"You're\", 'killin', 'it!'],\n",
       " 'LESSTHAN': ['Scrapers', 'for', 'the', 'win!'],\n",
       " 'DOLLAR': ['You', 'are', 'amazing!'],\n",
       " 'HASHTAG': ['Found', 'another', 'one!']}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the 4 hidden sentences together as a list of lists, then join them and print them with a for loop\n",
    "#save a clean version of the first chapter of Frankenstein to a text file (call it 'Clean_Frankenstein.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.lang.en import English # Create the nlp object\n",
    "import spacy\n",
    "from spacy.tokens import Doc,Span,Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp.make_doc(text_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'remove_bookend_tokens_from_doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-b0a49caa818d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bookend_spans'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_bookend_spans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#put labels in for spans and tokesn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mDoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text_with_no_bookends'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_bookend_tokens_from_doc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#bag of words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bookend_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#spacy recommends getter for Spans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bookend_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_bookend_tokens_from_doc' is not defined"
     ]
    }
   ],
   "source": [
    "#search/manipulation tasks with bookends\n",
    "Doc.set_extension('bookend_spans',method=get_bookend_spans,force=True) #put labels in for spans and tokesn\n",
    "Doc.set_extension('text_with_no_bookends',method=remove_bookend_tokens_from_doc)\n",
    "#summary tasks\n",
    "Doc.set_extension('bag_of_words',getter=get_bag_of_words)\n",
    "Doc.set_extension('word_length_plot',getter=plot_word_length)\n",
    "#span & token bookend properties populated by doc bookend tasks\n",
    "Span.set_extension('bookend_name',getter=lambda span,name: name,force=True) #spacy recommends getter for Spans\n",
    "Token.set_extension('bookend_name',default=None,force=True)\n",
    "Token.set_extension('is_in_bookend',default=False,force=True) #for tokens within bookends\n",
    "Token.set_extension('is_bookend',default=False,force=True) #for bookend tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bookend_spans(doc,bookend_key):\n",
    "    '''\n",
    "    create booleans for bookend span and type of bookend span\n",
    "     \n",
    "    used for the various tasks\n",
    "    '''\n",
    "    #initiate dict to store bookend spans in\n",
    "    bookend_spans = {name:[] for name in bookend_key.keys()}\n",
    "    \n",
    "    #get spans for each type of bookend\n",
    "    for name,symbol in bookend_key.items():\n",
    "        span_list_tmp = [] #initiate container to store spans\n",
    "        for match in re.finditer(f\"\\{symbol}.*\\{symbol}\",doc.text):\n",
    "            match_indices = match.span()\n",
    "            start,end = match_indices\n",
    "            \n",
    "            #Create a `Span` object from the slice `doc.text[start : end]`.\n",
    "            span = doc.char_span(start, end,label=name+str(start))\n",
    "            span._.bookend_name = name\n",
    "            span_list_tmp.append(span)\n",
    "            \n",
    "            #label tokens within span for easier reference in doc object \n",
    "            #(ie token --> doc based on span membership)\n",
    "            for token in span:\n",
    "                token._.bookend_name = name\n",
    "                if token.text==symbol:\n",
    "                    token._.is_bookend = True\n",
    "                else:\n",
    "                    token._.is_in_bookend = True\n",
    "            \n",
    "        bookend_tokens[name] \n",
    "    return bookend_spans\n",
    "\n",
    "def remove_bookend_tokens_from_text(doc):\n",
    "    '''\n",
    "    Removes the 4 hidden sentences from the original text file\n",
    "    \n",
    "    returns the text attribute with removal.\n",
    "    \n",
    "    TODO: save a clean version of the first chapter of Frankenstein to a text file (call it 'Clean_Frankenstein.txt')\n",
    "    '''\n",
    "    text_list_no_book_ends = [\n",
    "        token.text+token.whitespace_\n",
    "        for token in doc \n",
    "        if (not token._.is_bookend \n",
    "         and not token._.is_in_bookend)\n",
    "    ]\n",
    "    return ''.join(text_list_no_book_ends)\n",
    "\n",
    "\n",
    "def make_bag_of_words(doc):\n",
    "    '''\n",
    "    Stores all distinct words in the text in a dictionary \n",
    "    and keeps track of the word counts \n",
    "    (make sure to strip all punctuation and lower the case of the words)\n",
    "    \n",
    "    - assumes \"punctuation\" entails both classic punctuation characters and bookend characters\n",
    "    - alternatively, the token.is_punct_ attribute could be used (and may be more efficient)\n",
    "    \n",
    "    TODO: Save the distinct word dictionary as a JSON file (call it 'word_count_dict.json')\n",
    "    '''\n",
    "    text_tokenized = [\n",
    "        token.text.lower()\n",
    "        for token in doc\n",
    "        if (not token.text not in punctuation_chars \n",
    "            and token.text not in bookend_chars)\n",
    "    ]\n",
    "    return Counter(text_tokenized)\n",
    "\n",
    "def plot_word_length(doc):\n",
    "    \n",
    "    '''\n",
    "    Plot a histogram of the count of words by word length \n",
    "    for all words in the document\n",
    "    '''\n",
    "    \n",
    "    #get the word counts for each word length\n",
    "    word_lengths = [len(token.text)\n",
    "        for token in doc\n",
    "        if (not token.text not in punctuation_chars \n",
    "            and token.text not in bookend_chars)\n",
    "    ]\n",
    "    bag_of_lengths = pd.Series(Counter(word_lengths)).reset_index()\n",
    "    bag_of_lengths.columns = ['word_length','word_count']\n",
    "    \n",
    "    #plot the word length by the word count for each length\n",
    "    fig_word_count_by_word_length = bag_of_lengths.plot(kind='bar')\n",
    "    plt.title(\"Count of Words by Word Length \")\n",
    "    plt.xlabel(\"Word Length\")\n",
    "    plt.ylabel(\"Number of Words (Count)\\nat a Word Length\")\n",
    "    \n",
    "    return fig_word_count_by_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "bookend_spans = doc._.bookend_spans(bookend_symbols)\n",
    "for name,spans in bookend_spans.items():\n",
    "    \n",
    "#bag_of_words = \n",
    "#word_length_fig = \n",
    "#clean_text_no_bookends = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* False\n",
      "You True\n",
      "'re True\n",
      "* False\n",
      "* False\n",
      "killin True\n",
      "* False\n",
      "* False\n",
      "it True\n",
      "! True\n",
      "* False\n"
     ]
    }
   ],
   "source": [
    "for span in test['ASTERIX']:\n",
    "    for token in span:\n",
    "        print(token.text,token._.is_in_bookend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicDoc(Doc):\n",
    "    '''\n",
    "    a sub-class of Doc objects that allows removal of entities \n",
    "    from text and re-instantiate doc instance\n",
    "    '''\n",
    "    def __init__():\n",
    "        pass\n",
    "    \n",
    "    def remove_entities():\n",
    "        pass\n",
    "    \n",
    "    def \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Token.set_extension('is_in_bookend',default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Span(doc,0,3,label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy: create booleans for bookend span and type of bookend span\n",
    "for name,symbol in bookend_symbols.items():\n",
    "    for match in re.finditer(f\"\\{symbol}.*\\{symbol}\",doc.text):\n",
    "        match_indices = match.span()\n",
    "        start,end = match_indices\n",
    "        match_text = text_str[start:end]\n",
    "        span = doc.char_span(start, end)\n",
    "        if span is not None:\n",
    "            print(f\"Found Match for {name} of {span.text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
